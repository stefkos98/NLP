{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Preprocessing(reading the dataset, tokenization, stopwords elimination,\n",
    "#   noise elimination, transforming big letters, stemming, lemmatizing, spelling mistakes correcting)\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "\n",
    "#stopwords\n",
    "stwords = stopwords.words('english')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#corpus reader\n",
    "corpus_path= 'F:\\\\Literatura za faks\\\\MASTER\\\\NLP\\\\Sekspir'\n",
    "corpuses = PlaintextCorpusReader(corpus_path, '.*.txt')\n",
    "\n",
    "#speller\n",
    "speller = Speller(lang='en')\n",
    "\n",
    "#raw text\n",
    "shakespeare_text = corpuses.raw('sentences.txt')\n",
    "modern_text = corpuses.raw('modern_sentences.txt')\n",
    "\n",
    "#sentences\n",
    "shakespeare_sentences = sent_tokenize(shakespeare_text)\n",
    "modern_sentences = sent_tokenize(modern_text)\n",
    "\n",
    "#remove non-text from sentences\n",
    "#tokenize sentences\n",
    "processed_shakespeare_sentences_s = []\n",
    "processed_modern_sentences_s = []\n",
    "\n",
    "processed_shakespeare_sentences_l = []\n",
    "processed_modern_sentences_l = []\n",
    "\n",
    "#processing all shakespeare sentences\n",
    "for sentence in shakespeare_sentences:\n",
    "    sent = re.sub('[^A-Za-z]', ' ', sentence) #remove non-text\n",
    "    sent = sent.lower() #lower all letters\n",
    "    sent = word_tokenize(sent) #tokenize sentence\n",
    "    processed_sent_s = [] #processed sentences stemmer\n",
    "    processed_sent_l = [] #processed sentences lemmatizer\n",
    "    \n",
    "    #processing one sentence\n",
    "    for word in sent:\n",
    "        spelled_word = speller(word)\n",
    "        if spelled_word not in english_stopwords:\n",
    "            transformed_word_s = stemmer.stem(spelled_word) #stemming\n",
    "            transformed_word_l = lemmatizer.lemmatize(spelled_word) #lemmatizing\n",
    "            processed_sent_s.append(transformed_word_s) #stemmed words from one sentence\n",
    "            processed_sent_l.append(transformed_word_l) #lemmatized words from one sentence\n",
    "\n",
    "    #wrap up words into a sentence\n",
    "    processed_sent_s = \" \".join(processed_sent_s)\n",
    "    processed_sent_l = \" \".join(processed_sent_l)\n",
    "    \n",
    "    #save the sentence\n",
    "    processed_shakespeare_sentences_s.append(processed_sent_s)\n",
    "    processed_shakespeare_sentences_l.append(processed_sent_l)\n",
    "\n",
    "#processing all modern sentences\n",
    "for sentence in modern_sentences:\n",
    "    sent = re.sub('[^A-Za-z]', ' ', sentence) #remove non-text\n",
    "    sent = sent.lower() #lower all letters\n",
    "    sent = word_tokenize(sent) #tokenize sentence\n",
    "    processed_sent_s = [] #processed sentences stemmer\n",
    "    processed_sent_l = [] #processed sentences lemmatizer\n",
    "    \n",
    "    #processing one sentence\n",
    "    for word in sent:\n",
    "        #spelled_word = speller(word)\n",
    "        spelled_word =word\n",
    "        if spelled_word not in english_stopwords:\n",
    "            transformed_word_s = stemmer.stem(spelled_word) #stemming\n",
    "            transformed_word_l = lemmatizer.lemmatize(spelled_word) #lemmatizing\n",
    "            processed_sent_s.append(transformed_word_s) #stemmed words from one sentence\n",
    "            processed_sent_l.append(transformed_word_l) #lemmatized words from one sentence\n",
    "    \n",
    "    #wrap up words into a sentence\n",
    "    processed_sent_s = \" \".join(processed_sent_s)\n",
    "    processed_sent_l = \" \".join(processed_sent_l)\n",
    "    \n",
    "    #save the sentence\n",
    "    processed_modern_sentences_s.append(processed_sent_s)\n",
    "    processed_modern_sentences_l.append(processed_sent_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'ah': 0, 'alarm': 0, 'answer': 0, 'arm': 0, 'art': 0, 'ask': 0, 'away': 0, 'ay': 0, 'back': 0, 'bear': 0, 'beauti': 0, 'best': 0, 'better': 0, 'blood': 0, 'bodi': 0, 'boy': 0, 'brave': 0, 'break': 0, 'bring': 0, 'brother': 0, 'call': 0, 'care': 0, 'clifford': 0, 'come': 0, 'could': 0, 'cousin': 0, 'cri': 0, 'crown': 0, 'day': 0, 'dead': 0, 'death': 0, 'die': 0, 'done': 0, 'draw': 0, 'duke': 0, 'earl': 0, 'eight': 0, 'either': 0, 'els': 0, 'end': 0, 'enemi': 0, 'england': 0, 'enter': 0, 'er': 0, 'even': 0, 'event': 0, 'ever': 0, 'everi': 0, 'eye': 0, 'face': 0, 'faith': 0, 'fall': 0, 'far': 0, 'farewel': 0, 'father': 0, 'fear': 0, 'fight': 0, 'find': 0, 'first': 0, 'five': 0, 'fli': 0, 'follow': 0, 'foot': 0, 'four': 0, 'franc': 0, 'friend': 0, 'full': 0, 'game': 0, 'gentl': 0, 'get': 0, 'give': 0, 'gloucest': 0, 'go': 0, 'god': 0, 'good': 0, 'got': 1, 'grace': 0, 'great': 0, 'hand': 0, 'hang': 0, 'happi': 0, 'hast': 0, 'hath': 0, 'head': 0, 'hear': 0, 'heart': 0, 'heaven': 0, 'help': 0, 'henri': 0, 'hold': 0, 'home': 0, 'honour': 0, 'hope': 0, 'hors': 0, 'hour': 0, 'hous': 0, 'humphrey': 0, 'jack': 0, 'john': 0, 'keep': 0, 'king': 0, 'know': 0, 'ladi': 0, 'last': 0, 'leav': 0, 'left': 0, 'let': 0, 'lie': 0, 'life': 0, 'like': 0, 'littl': 0, 'live': 0, 'long': 0, 'look': 0, 'lord': 0, 'lost': 0, 'love': 0, 'made': 0, 'majesti': 0, 'make': 0, 'man': 0, 'mani': 0, 'march': 0, 'master': 0, 'may': 0, 'mean': 0, 'meet': 0, 'men': 0, 'might': 0, 'mind': 0, 'mine': 0, 'mortim': 0, 'mother': 0, 'much': 0, 'must': 0, 'name': 0, 'nay': 0, 'need': 0, 'never': 0, 'new': 0, 'news': 0, 'next': 0, 'night': 0, 'nine': 0, 'nobl': 0, 'none': 0, 'noth': 0, 'old': 0, 'one': 0, 'open': 0, 'part': 0, 'peac': 0, 'peopl': 0, 'perci': 0, 'place': 0, 'play': 0, 'pleas': 0, 'point': 0, 'poor': 0, 'power': 0, 'princ': 0, 'prison': 0, 'protector': 0, 'put': 0, 'queen': 0, 'reason': 0, 'rest': 0, 'richard': 0, 'right': 0, 'room': 0, 'run': 0, 'saw': 0, 'say': 0, 'see': 0, 'seem': 0, 'set': 0, 'seven': 0, 'shall': 0, 'shame': 0, 'show': 0, 'side': 0, 'sir': 0, 'six': 0, 'soldier': 0, 'somerset': 0, 'someth': 0, 'son': 0, 'soul': 0, 'sound': 0, 'speak': 0, 'spirit': 0, 'st': 0, 'stand': 0, 'state': 0, 'stay': 0, 'still': 0, 'suffolk': 0, 'sure': 0, 'sweet': 0, 'sword': 0, 'take': 0, 'talent': 0, 'talk': 0, 'tear': 0, 'tell': 0, 'thank': 0, 'thee': 0, 'therefor': 0, 'thi': 0, 'thing': 0, 'think': 0, 'thou': 0, 'though': 0, 'thought': 0, 'thousand': 0, 'three': 0, 'thu': 0, 'ti': 0, 'till': 0, 'time': 0, 'tongu': 0, 'traitor': 0, 'true': 0, 'turn': 0, 'two': 0, 'uncl': 0, 'unto': 0, 'upon': 0, 'us': 0, 'use': 0, 'walk': 0, 'want': 0, 'war': 0, 'warwick': 0, 'way': 0, 'well': 0, 'went': 0, 'whose': 0, 'woman': 0, 'word': 0, 'work': 0, 'world': 0, 'would': 0, 'wrong': 0, 'ye': 0, 'year': 0, 'yet': 0, 'york': 0, 'young': 0, 'zero': 0}, 'modern')\n",
      "({'ah': 0, 'alarm': 0, 'answer': 0, 'arm': 0, 'art': 0, 'aside': 0, 'away': 0, 'ay': 0, 'back': 0, 'bear': 0, 'best': 0, 'better': 0, 'blood': 0, 'body': 0, 'boy': 0, 'brave': 0, 'break': 0, 'brother': 0, 'call': 0, 'care': 0, 'clifford': 0, 'come': 0, 'could': 0, 'cousin': 0, 'crown': 0, 'day': 0, 'dead': 0, 'death': 0, 'devil': 0, 'die': 0, 'done': 0, 'draw': 0, 'duke': 0, 'earl': 0, 'earth': 0, 'eight': 0, 'either': 0, 'else': 0, 'end': 0, 'enemy': 0, 'england': 0, 'english': 0, 'enough': 0, 'enter': 0, 'er': 0, 'even': 0, 'event': 0, 'ever': 0, 'every': 0, 'exit': 0, 'eye': 0, 'face': 0, 'faith': 0, 'fall': 0, 'far': 0, 'farewell': 0, 'father': 0, 'fear': 0, 'fight': 0, 'find': 0, 'first': 0, 'five': 0, 'fly': 0, 'foot': 0, 'four': 0, 'france': 0, 'french': 0, 'friend': 0, 'full': 0, 'game': 0, 'gentle': 0, 'get': 0, 'give': 0, 'gloucester': 0, 'go': 0, 'god': 0, 'going': 0, 'good': 0, 'got': 0, 'grace': 0, 'great': 0, 'hand': 0, 'hast': 0, 'hath': 0, 'head': 0, 'hear': 0, 'heart': 0, 'heaven': 0, 'help': 0, 'henry': 0, 'hold': 0, 'home': 0, 'honour': 0, 'hope': 0, 'horse': 0, 'hour': 0, 'house': 0, 'humphrey': 0, 'jack': 0, 'john': 0, 'keep': 0, 'king': 0, 'knee': 0, 'know': 0, 'lady': 0, 'last': 0, 'leave': 0, 'left': 0, 'let': 0, 'life': 0, 'like': 0, 'little': 0, 'live': 0, 'long': 0, 'look': 0, 'looked': 0, 'lord': 0, 'lost': 0, 'love': 0, 'made': 0, 'majesty': 0, 'make': 0, 'man': 0, 'many': 0, 'march': 0, 'master': 0, 'may': 0, 'mean': 0, 'meet': 0, 'men': 0, 'might': 0, 'mind': 0, 'mine': 0, 'mortimer': 0, 'mother': 0, 'much': 0, 'must': 0, 'name': 0, 'nay': 0, 'need': 0, 'never': 0, 'new': 0, 'news': 0, 'next': 0, 'night': 2, 'nine': 0, 'noble': 0, 'none': 0, 'nothing': 0, 'old': 0, 'one': 0, 'part': 0, 'peace': 0, 'people': 0, 'percy': 0, 'place': 0, 'play': 0, 'please': 0, 'point': 0, 'poor': 0, 'power': 0, 'prince': 0, 'protector': 0, 'proud': 0, 'put': 0, 'queen': 0, 'reason': 0, 'rest': 0, 'richard': 0, 'right': 0, 'room': 0, 'run': 0, 'saw': 0, 'say': 0, 'see': 0, 'set': 0, 'seven': 0, 'shall': 0, 'shalt': 0, 'shame': 0, 'show': 0, 'side': 0, 'sir': 0, 'six': 0, 'soldier': 0, 'somerset': 0, 'something': 0, 'son': 0, 'soul': 0, 'sound': 0, 'speak': 0, 'spirit': 0, 'st': 0, 'stand': 0, 'state': 0, 'stay': 0, 'still': 0, 'suffolk': 0, 'sure': 0, 'sweet': 0, 'sword': 0, 'take': 0, 'talent': 0, 'tear': 0, 'tell': 0, 'thee': 0, 'therefore': 0, 'thing': 0, 'think': 0, 'thou': 0, 'though': 0, 'thought': 0, 'thousand': 0, 'three': 0, 'thus': 0, 'thy': 0, 'ti': 0, 'till': 0, 'time': 0, 'tongue': 0, 'true': 0, 'turn': 0, 'turned': 0, 'two': 0, 'uncle': 0, 'unto': 0, 'upon': 0, 'want': 0, 'wanted': 0, 'war': 0, 'warwick': 0, 'way': 0, 'well': 0, 'went': 0, 'whose': 0, 'wilt': 0, 'within': 0, 'woman': 0, 'word': 0, 'work': 0, 'world': 0, 'would': 0, 'wrong': 0, 'ye': 0, 'year': 0, 'yes': 0, 'yet': 0, 'york': 0, 'young': 0, 'zero': 0}, 'modern')\n"
     ]
    }
   ],
   "source": [
    "#2. Attribute extraction by using Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix_s = CountVectorizer(max_features=250)\n",
    "matrix_l = CountVectorizer(max_features=250)\n",
    "\n",
    "processed_sentences_s = processed_shakespeare_sentences_s + processed_modern_sentences_s\n",
    "processed_sentences_l = processed_shakespeare_sentences_l + processed_modern_sentences_l\n",
    "\n",
    "X_s = matrix_s.fit_transform(processed_sentences_s).toarray()\n",
    "X_l= matrix_l.fit_transform(processed_sentences_l).toarray()\n",
    "y = ['shakespeare'] * len(processed_shakespeare_sentences_s) + ['modern'] * len(processed_modern_sentences_s)\n",
    "\n",
    "#3. Naive Bayes Classification\n",
    "feature_names_s = matrix_s.get_feature_names()\n",
    "feature_names_l = matrix_l.get_feature_names()\n",
    "# (features, label)\n",
    "\n",
    "def generate_features_s(vector):\n",
    "    dictionary = {}\n",
    "    for ind, number in enumerate(vector):\n",
    "        key = feature_names_s[ind]\n",
    "        dictionary[key] = number\n",
    "    return dictionary\n",
    "\n",
    "def generate_features_l(vector):\n",
    "    dictionary = {}\n",
    "    for ind, number in enumerate(vector):\n",
    "        key = feature_names_l[ind]\n",
    "        dictionary[key] = number\n",
    "    return dictionary\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "X_nltk_s = []\n",
    "X_nltk_l = []\n",
    "for ind, x in enumerate(X_s):\n",
    "    X_nltk_s.append((generate_features_s(x), y[ind]))\n",
    "for ind, x in enumerate(X_l):\n",
    "    X_nltk_l.append((generate_features_l(x), y[ind]))\n",
    "    \n",
    "#randomize set\n",
    "import random\n",
    "random.shuffle(X_nltk_s)\n",
    "random.shuffle(X_nltk_l)\n",
    "number_of_entries = len(X_nltk_s)\n",
    "half_of_entries = number_of_entries // 2\n",
    "train_set_s, test_set_s = X_nltk_s[half_of_entries:], X_nltk_s[:half_of_entries]\n",
    "train_set_l, test_set_l = X_nltk_l[half_of_entries:], X_nltk_l[:half_of_entries]\n",
    "\n",
    "print(train_set_s[0])\n",
    "print(train_set_l[0])\n",
    "classifierNLTK_s = NaiveBayesClassifier.train(train_set_s)\n",
    "classifierNLTK_l = NaiveBayesClassifier.train(train_set_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_s = TfidfVectorizer(max_features=250)\n",
    "tf_l = TfidfVectorizer(max_features=250)\n",
    "\n",
    "text_tf_s = tf_s.fit_transform(processed_sentences_s)\n",
    "text_tf_l = tf_l.fit_transform(processed_sentences_l)\n",
    "\n",
    "feature_names_tf_s = tf_s.get_feature_names()\n",
    "feature_names_tf_l = tf_l.get_feature_names()\n",
    "\n",
    "def generate_features_tf_s(vector):\n",
    "    dictionary = {}\n",
    "    for ind, number in enumerate(vector):\n",
    "        key = feature_names_tf_s[ind]\n",
    "        dictionary[key] = number\n",
    "    return dictionary\n",
    "\n",
    "def generate_features_tf_l(vector):\n",
    "    dictionary = {}\n",
    "    for ind, number in enumerate(vector):\n",
    "        key = feature_names_tf_l[ind]\n",
    "        dictionary[key] = number\n",
    "    return dictionary\n",
    "\n",
    "data_tf_s = []\n",
    "data_tf_l = []\n",
    "for i in range(text_tf_s.shape[0]):\n",
    "    vector = []\n",
    "    for j in range(text_tf_s.shape[1]):\n",
    "        vector.append(text_tf_s[i,j])\n",
    "    data_tf_s.append((generate_features_tf_s(vector), y[i]))\n",
    "for i in range(text_tf_l.shape[0]):\n",
    "    vector = []\n",
    "    for j in range(text_tf_l.shape[1]):\n",
    "        vector.append(text_tf_l[i,j])\n",
    "    data_tf_l.append((generate_features_tf_l(vector), y[i]))\n",
    "import random\n",
    "random.shuffle(data_tf_s)\n",
    "random.shuffle(data_tf_l)\n",
    "train_set_tf_s, test_set_tf_s = data_tf_s[half_of_entries:], data_tf_s[:half_of_entries]\n",
    "train_set_tf_l, test_set_tf_l = data_tf_l[half_of_entries:], data_tf_l[:half_of_entries]\n",
    "\n",
    "classifier_s = nltk.classify.NaiveBayesClassifier.train(train_set_tf_s)\n",
    "classifier_l = nltk.classify.NaiveBayesClassifier.train(train_set_tf_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer Bayes classificator accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.836101660996598"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Stemmer Bayes classificator accuracy')\n",
    "nltk.classify.accuracy(classifierNLTK_s, test_set_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer Bayes classificator accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8399039423654192"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Lemmatizer Bayes classificator accuracy')\n",
    "nltk.classify.accuracy(classifierNLTK_l, test_set_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5207124274564738"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_s, test_set_tf_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347208324994998"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_l, test_set_tf_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4997\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_tf_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierDT_s = nltk.classify.DecisionTreeClassifier.train(train_set_s)\n",
    "classifierDT_l = nltk.classify.DecisionTreeClassifier.train(train_set_l)\n",
    "classifierME_s = nltk.classify.MaxentClassifier.train(train_set_s, trace=0)\n",
    "classifierME_l = nltk.classify.MaxentClassifier.train(train_set_l, trace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierDT_s_IDF = nltk.classify.DecisionTreeClassifier.train(train_set_tf_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifierDT_l_IDF = nltk.classify.DecisionTreeClassifier.train(train_set_tf_l)\n",
    "classifierME_s_IDF = nltk.classify.MaxentClassifier.train(train_set_tf_s, trace=0)\n",
    "classifierME_l_IDF = nltk.classify.MaxentClassifier.train(train_set_tf_l, trace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8206924154492695"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierDT_s, test_set_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332999799879928"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierDT_l, test_set_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872723634180508"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierME_s, test_set_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808685211126676"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierME_l, test_set_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5461276766059636"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierDT_s_IDF, test_set_tf_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5875525315189113"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierDT_l_IDF, test_set_tf_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481288773263958"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierME_s_IDF, test_set_tf_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5539323594156494"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifierME_l_IDF, test_set_tf_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number 1 is shakespeare\n",
      "Sentence number 2 is modern\n",
      "Sentence number 3 is modern\n",
      "Sentence number 4 is modern\n",
      "Sentence number 5 is modern\n",
      "Sentence number 6 is modern\n",
      "Sentence number 7 is shakespeare\n",
      "Sentence number 8 is shakespeare\n",
      "Sentence number 9 is modern\n",
      "Sentence number 10 is modern\n",
      "Sentence number 11 is shakespeare\n"
     ]
    }
   ],
   "source": [
    "#shakespeare\n",
    "test_sentences1 = 'We will die all three: But I will prove that two kings are as good As him and me. '\n",
    "#modern\n",
    "test_sentences2 = 'Hello, how are you and what are you doing today, and will you go out with friends tonight? ' \n",
    "#modern\n",
    "test_sentences3 = 'One girl going to see nine dolphins. '\n",
    "#modern\n",
    "test_sentences4 = 'In that he went too far away. ' \n",
    "#modern\n",
    "test_sentences5 = 'AUTHORITIES CLAIMS THAT THE KILLINGS WERE THE crime OF ANGER. ' \n",
    "#modern\n",
    "test_sentences6 = 'I was very little when i lost my tooth. '\n",
    "#shakespeare\n",
    "test_sentences7 = 'But, gracious sir, Here are your sons again, and I must lose Two of the sweet\\'st companions in the world. '\n",
    "#modern\n",
    "test_sentences8 = 'King George didn\\'t fear anything and decided that he is going to see that spectacle. '\n",
    "#modern\n",
    "test_sentences9 = 'George six didn\\'t fear anything and he looked at something. '\n",
    "#modern\n",
    "test_sentences10= 'Panmure stabbing: Bradford Kipa pleads guilty to murder of John Tofu Ioane. '\n",
    "#modern\n",
    "test_sentences11= 'Panmure stabbing: Bradford Kipa pleads guilty to murder of John Tofu Ioane which died. '\n",
    "\n",
    "test_sentences = test_sentences1 + test_sentences2 + test_sentences3 + test_sentences4 + test_sentences5 + test_sentences6 + test_sentences7\n",
    "test_sentences = test_sentences + test_sentences8 + test_sentences9 + test_sentences10 + test_sentences11\n",
    "def test_sentences_func(input_sentences):\n",
    "    sentences = sent_tokenize(input_sentences)\n",
    "    #remove non-text from sentences\n",
    "    #tokenize sentences\n",
    "    sents_features_and_values = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent = re.sub('[^A-Za-z]', ' ', sentence) #remove non-text\n",
    "        sent = sent.lower() #lower all letters\n",
    "        sent = word_tokenize(sent) #tokenize sentence\n",
    "        features_and_values = {word : 0 for word in feature_names_l} #processed sentences stemmer\n",
    "    \n",
    "        #processing one sentence\n",
    "        for word in sent:\n",
    "            spelled_word = speller(word)\n",
    "            if spelled_word not in english_stopwords:\n",
    "                transformed_word = stemmer.stem(spelled_word) #stemming\n",
    "                if transformed_word in feature_names_l:\n",
    "                    features_and_values[transformed_word] = features_and_values[transformed_word] + 1\n",
    "        sents_features_and_values.append(features_and_values)   \n",
    "    for i in range(0, len(sentences)):\n",
    "        res = classifierNLTK_l.classify(sents_features_and_values[i])\n",
    "        print('Sentence number %d is' %(i + 1), res)\n",
    "test_sentences_func(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    king = 1              shakes : modern =     41.0 : 1.0\n",
      "                    fear = 1              shakes : modern =     30.6 : 1.0\n",
      "                   going = 1              modern : shakes =     29.6 : 1.0\n",
      "                  talent = 1              shakes : modern =     27.9 : 1.0\n",
      "               something = 1              modern : shakes =     24.1 : 1.0\n",
      "                  looked = 1              modern : shakes =     21.5 : 1.0\n",
      "                    mine = 1              shakes : modern =     20.8 : 1.0\n",
      "                   peace = 1              shakes : modern =     20.5 : 1.0\n",
      "                    fall = 1              shakes : modern =     19.8 : 1.0\n",
      "                    soul = 1              shakes : modern =     18.5 : 1.0\n",
      "                     set = 1              shakes : modern =     18.3 : 1.0\n",
      "                 soldier = 1              shakes : modern =     17.8 : 1.0\n",
      "                     son = 1              shakes : modern =     17.5 : 1.0\n",
      "                   speak = 1              shakes : modern =     17.5 : 1.0\n",
      "                     art = 1              shakes : modern =     17.1 : 1.0\n",
      "                    show = 1              shakes : modern =     16.5 : 1.0\n",
      "                  french = 1              shakes : modern =     15.8 : 1.0\n",
      "                   brave = 1              shakes : modern =     15.8 : 1.0\n",
      "                   horse = 1              shakes : modern =     15.8 : 1.0\n",
      "                   eight = 1              modern : shakes =     15.5 : 1.0\n",
      "                   stand = 1              shakes : modern =     15.5 : 1.0\n",
      "                   sound = 1              shakes : modern =     15.1 : 1.0\n",
      "                   fight = 1              shakes : modern =     14.7 : 1.0\n",
      "                   power = 1              shakes : modern =     14.4 : 1.0\n",
      "                  spirit = 1              shakes : modern =     13.8 : 1.0\n",
      "                    body = 1              shakes : modern =     13.1 : 1.0\n",
      "                     god = 1              shakes : modern =     13.1 : 1.0\n",
      "                  master = 1              shakes : modern =     12.7 : 1.0\n",
      "                   event = 1              shakes : modern =     11.9 : 1.0\n",
      "                     die = 1              shakes : modern =     10.7 : 1.0\n",
      "                     six = 1              modern : shakes =     10.5 : 1.0\n",
      "                     sir = 1              shakes : modern =     10.5 : 1.0\n",
      "                    tear = 1              shakes : modern =     10.4 : 1.0\n",
      "                   alarm = 1              shakes : modern =     10.3 : 1.0\n",
      "                thousand = 1              shakes : modern =     10.3 : 1.0\n",
      "                  turned = 1              modern : shakes =     10.1 : 1.0\n",
      "                     men = 1              shakes : modern =     10.0 : 1.0\n",
      "                   proud = 1              shakes : modern =      9.9 : 1.0\n",
      "                    bear = 1              shakes : modern =      9.3 : 1.0\n",
      "                    till = 1              shakes : modern =      9.1 : 1.0\n",
      "                   enemy = 1              shakes : modern =      9.1 : 1.0\n",
      "                   every = 1              shakes : modern =      8.7 : 1.0\n",
      "                    rest = 1              shakes : modern =      8.7 : 1.0\n",
      "                    nine = 1              modern : shakes =      8.6 : 1.0\n",
      "                    dead = 1              shakes : modern =      8.5 : 1.0\n",
      "                      ah = 1              shakes : modern =      8.3 : 1.0\n",
      "                   uncle = 1              shakes : modern =      8.3 : 1.0\n",
      "                   shall = 1              shakes : modern =      8.1 : 1.0\n",
      "                    lady = 1              shakes : modern =      7.9 : 1.0\n",
      "                  cousin = 1              shakes : modern =      7.9 : 1.0\n",
      "                 english = 1              shakes : modern =      7.9 : 1.0\n",
      "                   death = 1              shakes : modern =      7.8 : 1.0\n",
      "                    call = 1              shakes : modern =      7.4 : 1.0\n",
      "                    poor = 1              shakes : modern =      7.3 : 1.0\n",
      "                    meet = 1              shakes : modern =      7.3 : 1.0\n",
      "                    went = 1              modern : shakes =      7.3 : 1.0\n",
      "                   faith = 1              shakes : modern =      7.3 : 1.0\n",
      "                   sweet = 1              shakes : modern =      7.1 : 1.0\n",
      "                    true = 1              shakes : modern =      6.6 : 1.0\n",
      "                    mean = 1              shakes : modern =      6.3 : 1.0\n",
      "                   state = 1              shakes : modern =      6.2 : 1.0\n",
      "                   heart = 1              shakes : modern =      6.1 : 1.0\n",
      "                  father = 1              shakes : modern =      6.0 : 1.0\n",
      "                   earth = 1              shakes : modern =      5.8 : 1.0\n",
      "                   three = 2              modern : shakes =      5.6 : 1.0\n",
      "                    word = 1              shakes : modern =      5.6 : 1.0\n",
      "                    want = 1              modern : shakes =      5.5 : 1.0\n",
      "                   shame = 1              shakes : modern =      5.3 : 1.0\n",
      "                  people = 1              modern : shakes =      5.2 : 1.0\n",
      "                    room = 1              modern : shakes =      5.2 : 1.0\n",
      "                    come = 2              shakes : modern =      5.0 : 1.0\n",
      "                  gentle = 1              shakes : modern =      5.0 : 1.0\n",
      "                   young = 1              shakes : modern =      5.0 : 1.0\n",
      "                     two = 2              modern : shakes =      5.0 : 1.0\n",
      "                     get = 1              modern : shakes =      5.0 : 1.0\n",
      "                    face = 1              shakes : modern =      4.9 : 1.0\n",
      "                 brother = 1              shakes : modern =      4.8 : 1.0\n",
      "                     yet = 1              shakes : modern =      4.8 : 1.0\n",
      "                     arm = 1              shakes : modern =      4.4 : 1.0\n",
      "                    like = 2              shakes : modern =      4.4 : 1.0\n",
      "                    five = 2              modern : shakes =      4.3 : 1.0\n",
      "                     war = 1              shakes : modern =      4.2 : 1.0\n",
      "                   break = 1              shakes : modern =      4.1 : 1.0\n",
      "                    head = 1              shakes : modern =      4.1 : 1.0\n",
      "                    hear = 1              shakes : modern =      4.1 : 1.0\n",
      "                    john = 1              shakes : modern =      4.0 : 1.0\n",
      "                   point = 1              shakes : modern =      3.9 : 1.0\n",
      "                    game = 1              modern : shakes =      3.9 : 1.0\n",
      "                   leave = 1              shakes : modern =      3.8 : 1.0\n",
      "                      go = 2              shakes : modern =      3.7 : 1.0\n",
      "                     day = 2              shakes : modern =      3.7 : 1.0\n",
      "                    live = 1              shakes : modern =      3.6 : 1.0\n",
      "                    name = 1              shakes : modern =      3.6 : 1.0\n",
      "                   would = 2              shakes : modern =      3.4 : 1.0\n",
      "                  wanted = 1              modern : shakes =      3.4 : 1.0\n",
      "                    four = 1              modern : shakes =      3.4 : 1.0\n",
      "                    next = 1              shakes : modern =      3.3 : 1.0\n",
      "                     may = 1              shakes : modern =      3.3 : 1.0\n",
      "                  though = 1              shakes : modern =      3.3 : 1.0\n",
      "                    stay = 1              shakes : modern =      3.2 : 1.0\n",
      "                    five = 1              modern : shakes =      3.2 : 1.0\n",
      "                     boy = 1              shakes : modern =      3.2 : 1.0\n",
      "                    foot = 1              shakes : modern =      3.2 : 1.0\n",
      "                   seven = 1              modern : shakes =      3.1 : 1.0\n",
      "                  enough = 1              shakes : modern =      3.0 : 1.0\n",
      "                    hour = 1              shakes : modern =      3.0 : 1.0\n",
      "                    life = 1              shakes : modern =      3.0 : 1.0\n",
      "                     let = 1              shakes : modern =      3.0 : 1.0\n",
      "                    come = 1              shakes : modern =      3.0 : 1.0\n",
      "                     run = 1              shakes : modern =      2.9 : 1.0\n",
      "                    well = 1              shakes : modern =      2.8 : 1.0\n",
      "                    ever = 1              shakes : modern =      2.8 : 1.0\n",
      "                     got = 1              modern : shakes =      2.7 : 1.0\n",
      "                    lost = 1              shakes : modern =      2.7 : 1.0\n",
      "                    full = 1              shakes : modern =      2.7 : 1.0\n",
      "                    make = 1              shakes : modern =      2.6 : 1.0\n",
      "                    four = 2              modern : shakes =      2.6 : 1.0\n",
      "                    work = 1              modern : shakes =      2.6 : 1.0\n",
      "                    give = 1              shakes : modern =      2.6 : 1.0\n",
      "                    need = 1              modern : shakes =      2.5 : 1.0\n",
      "                    none = 1              shakes : modern =      2.5 : 1.0\n",
      "                   think = 1              shakes : modern =      2.4 : 1.0\n",
      "                   wrong = 1              shakes : modern =      2.4 : 1.0\n",
      "                  answer = 1              shakes : modern =      2.4 : 1.0\n",
      "                    back = 1              modern : shakes =      2.4 : 1.0\n",
      "                    made = 1              shakes : modern =      2.4 : 1.0\n",
      "                 nothing = 1              modern : shakes =      2.4 : 1.0\n",
      "                    away = 1              shakes : modern =      2.3 : 1.0\n",
      "                     old = 1              shakes : modern =      2.3 : 1.0\n",
      "                   might = 1              shakes : modern =      2.3 : 1.0\n",
      "                     man = 1              shakes : modern =      2.3 : 1.0\n",
      "                   never = 1              shakes : modern =      2.3 : 1.0\n",
      "                     put = 1              modern : shakes =      2.3 : 1.0\n",
      "                   three = 1              modern : shakes =      2.2 : 1.0\n",
      "                     one = 2              modern : shakes =      2.2 : 1.0\n",
      "                    take = 1              shakes : modern =      2.1 : 1.0\n",
      "                    done = 1              shakes : modern =      2.1 : 1.0\n",
      "                     eye = 1              shakes : modern =      2.1 : 1.0\n",
      "                   woman = 1              shakes : modern =      2.1 : 1.0\n",
      "                    many = 1              shakes : modern =      2.1 : 1.0\n",
      "                    draw = 1              modern : shakes =      2.1 : 1.0\n",
      "                  please = 1              shakes : modern =      2.0 : 1.0\n",
      "                    news = 1              shakes : modern =      1.9 : 1.0\n",
      "                     say = 1              shakes : modern =      1.9 : 1.0\n",
      "                    even = 1              shakes : modern =      1.9 : 1.0\n",
      "                    hope = 1              shakes : modern =      1.9 : 1.0\n",
      "                    keep = 1              shakes : modern =      1.9 : 1.0\n",
      "                     new = 1              modern : shakes =      1.9 : 1.0\n",
      "                    turn = 1              shakes : modern =      1.9 : 1.0\n",
      "                   world = 1              shakes : modern =      1.8 : 1.0\n",
      "                     way = 1              modern : shakes =      1.8 : 1.0\n",
      "                   thing = 1              modern : shakes =      1.8 : 1.0\n",
      "                     one = 1              modern : shakes =      1.8 : 1.0\n",
      "                   house = 1              shakes : modern =      1.8 : 1.0\n",
      "                   could = 1              modern : shakes =      1.7 : 1.0\n",
      "                    left = 1              modern : shakes =      1.7 : 1.0\n",
      "                    long = 1              shakes : modern =      1.7 : 1.0\n",
      "                     yes = 1              modern : shakes =      1.7 : 1.0\n",
      "                     saw = 1              modern : shakes =      1.7 : 1.0\n",
      "                    like = 1              modern : shakes =      1.7 : 1.0\n",
      "                    know = 2              modern : shakes =      1.7 : 1.0\n",
      "                  friend = 1              shakes : modern =      1.6 : 1.0\n",
      "                     far = 1              shakes : modern =      1.6 : 1.0\n",
      "                     day = 1              shakes : modern =      1.6 : 1.0\n",
      "                    jack = 1              shakes : modern =      1.6 : 1.0\n",
      "                    part = 1              shakes : modern =      1.6 : 1.0\n",
      "                    know = 1              modern : shakes =      1.5 : 1.0\n",
      "                  mother = 1              shakes : modern =      1.5 : 1.0\n",
      "                     see = 1              modern : shakes =      1.5 : 1.0\n",
      "                    year = 1              modern : shakes =      1.5 : 1.0\n",
      "                   night = 1              shakes : modern =      1.5 : 1.0\n",
      "                     two = 1              modern : shakes =      1.5 : 1.0\n",
      "                    else = 1              shakes : modern =      1.4 : 1.0\n",
      "                    care = 1              shakes : modern =      1.4 : 1.0\n",
      "                    side = 1              shakes : modern =      1.4 : 1.0\n",
      "                 thought = 1              shakes : modern =      1.4 : 1.0\n",
      "                  reason = 1              shakes : modern =      1.4 : 1.0\n",
      "                    hand = 1              shakes : modern =      1.4 : 1.0\n",
      "                   great = 1              shakes : modern =      1.4 : 1.0\n",
      "                    time = 1              modern : shakes =      1.3 : 1.0\n",
      "                   first = 1              shakes : modern =      1.3 : 1.0\n",
      "                    love = 1              modern : shakes =      1.3 : 1.0\n",
      "                  better = 1              modern : shakes =      1.3 : 1.0\n",
      "                   right = 1              modern : shakes =      1.3 : 1.0\n",
      "                    best = 1              modern : shakes =      1.3 : 1.0\n",
      "                  little = 1              modern : shakes =      1.2 : 1.0\n",
      "                    home = 1              shakes : modern =      1.2 : 1.0\n",
      "                    help = 1              shakes : modern =      1.2 : 1.0\n",
      "                    find = 1              shakes : modern =      1.2 : 1.0\n",
      "                    much = 1              shakes : modern =      1.1 : 1.0\n",
      "                   place = 1              shakes : modern =      1.1 : 1.0\n",
      "                     end = 1              modern : shakes =      1.1 : 1.0\n",
      "                    thou = 0              modern : shakes =      1.1 : 1.0\n",
      "                    lord = 0              modern : shakes =      1.1 : 1.0\n",
      "                    mind = 1              modern : shakes =      1.1 : 1.0\n",
      "                    look = 1              modern : shakes =      1.1 : 1.0\n",
      "                     thy = 0              modern : shakes =      1.1 : 1.0\n",
      "                   would = 1              modern : shakes =      1.1 : 1.0\n",
      "                   shall = 0              modern : shakes =      1.1 : 1.0\n",
      "                    king = 0              modern : shakes =      1.1 : 1.0\n",
      "                    play = 1              modern : shakes =      1.1 : 1.0\n",
      "                    sure = 1              modern : shakes =      1.1 : 1.0\n",
      "                   still = 1              shakes : modern =      1.1 : 1.0\n",
      "                    thee = 0              modern : shakes =      1.1 : 1.0\n",
      "                    tell = 1              shakes : modern =      1.1 : 1.0\n",
      "                    must = 1              shakes : modern =      1.1 : 1.0\n",
      "                    hath = 0              modern : shakes =      1.0 : 1.0\n",
      "                    upon = 0              modern : shakes =      1.0 : 1.0\n",
      "                    duke = 0              modern : shakes =      1.0 : 1.0\n",
      "                    york = 0              modern : shakes =      1.0 : 1.0\n",
      "                     let = 0              modern : shakes =      1.0 : 1.0\n",
      "                   going = 0              shakes : modern =      1.0 : 1.0\n",
      "                    come = 0              modern : shakes =      1.0 : 1.0\n",
      "                   death = 0              modern : shakes =      1.0 : 1.0\n",
      "                     god = 0              modern : shakes =      1.0 : 1.0\n",
      "                   henry = 0              modern : shakes =      1.0 : 1.0\n",
      "                  father = 0              modern : shakes =      1.0 : 1.0\n",
      "                      go = 1              shakes : modern =      1.0 : 1.0\n",
      "                    make = 0              modern : shakes =      1.0 : 1.0\n",
      "                     art = 0              modern : shakes =      1.0 : 1.0\n",
      "                    want = 0              shakes : modern =      1.0 : 1.0\n",
      "                   think = 0              modern : shakes =      1.0 : 1.0\n",
      "                    well = 0              modern : shakes =      1.0 : 1.0\n",
      "                    mine = 0              modern : shakes =      1.0 : 1.0\n",
      "                    life = 0              modern : shakes =      1.0 : 1.0\n",
      "                   heart = 0              modern : shakes =      1.0 : 1.0\n",
      "                  france = 0              modern : shakes =      1.0 : 1.0\n",
      "                     men = 0              modern : shakes =      1.0 : 1.0\n",
      "                   blood = 0              modern : shakes =      1.0 : 1.0\n",
      "                   crown = 0              modern : shakes =      1.0 : 1.0\n",
      "                    hast = 0              modern : shakes =      1.0 : 1.0\n",
      "                     man = 0              modern : shakes =      1.0 : 1.0\n",
      "                     may = 0              modern : shakes =      1.0 : 1.0\n",
      "                     son = 0              modern : shakes =      1.0 : 1.0\n",
      "                  talent = 0              modern : shakes =      1.0 : 1.0\n",
      "                    head = 0              modern : shakes =      1.0 : 1.0\n",
      "                     one = 0              shakes : modern =      1.0 : 1.0\n",
      "                    word = 0              modern : shakes =      1.0 : 1.0\n",
      "                    fear = 0              modern : shakes =      1.0 : 1.0\n",
      "                    unto = 0              modern : shakes =      1.0 : 1.0\n",
      "                     yet = 0              modern : shakes =      1.0 : 1.0\n",
      "                     set = 0              modern : shakes =      1.0 : 1.0\n",
      "                   grace = 0              modern : shakes =      1.0 : 1.0\n",
      "                   speak = 0              modern : shakes =      1.0 : 1.0\n",
      "                    thus = 0              modern : shakes =      1.0 : 1.0\n",
      "                    bear = 0              modern : shakes =      1.0 : 1.0\n",
      "               therefore = 0              modern : shakes =      1.0 : 1.0\n",
      "                    like = 0              shakes : modern =      1.0 : 1.0\n",
      "                      ti = 0              modern : shakes =      1.0 : 1.0\n",
      "                      st = 0              modern : shakes =      1.0 : 1.0\n",
      "                     get = 0              shakes : modern =      1.0 : 1.0\n",
      "                     sir = 0              modern : shakes =      1.0 : 1.0\n",
      "                   whose = 0              modern : shakes =      1.0 : 1.0\n",
      "                   stand = 0              modern : shakes =      1.0 : 1.0\n",
      "                      er = 0              modern : shakes =      1.0 : 1.0\n",
      "                   queen = 0              modern : shakes =      1.0 : 1.0\n",
      "                 richard = 0              modern : shakes =      1.0 : 1.0\n",
      "                     say = 0              modern : shakes =      1.0 : 1.0\n",
      "                    call = 0              modern : shakes =      1.0 : 1.0\n",
      "                   fight = 0              modern : shakes =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifierNLTK_l.show_most_informative_features(260)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
